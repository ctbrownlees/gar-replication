source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
sd(global.nfci)
global.nfci  <- scale(rowMeans(scale(nfci),na.rm=TRUE))
nfci[colSums(is.na(nfci))==nrow(nfci)] <- rowMeans(nfci,na.rm = TRUE)
for (i in 1:ncol(nfci)){
nfci[is.na(nfci[,i]),i] <- global.nfci[is.na(nfci[,i])] * sd(nfci[,i],na.rm = TRUE)
}
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
plot(nfci[,1])
sd(nfci[,1])
rm(list=ls())
library(readxl)
library(lubridate)
library(countrycode)
## First, we do "Rest of the countries"
name.range <- "A1:CU1"
names <- read_excel("../../data/raw/nfci.xlsx",sheet = 3,range = name.range)
names.idx <- seq(1,ncol(names),7)
names <- colnames(names)[names.idx]
data.range <- "A2:CY178"
data  <- read_excel("../../data/raw/nfci.xlsx",sheet = 3,range = data.range,)
idx <- names.idx + 4
dates <- data[,names.idx]
fci   <-  data[, idx]
dframe <- data.frame(dates[,ncol(dates)])
dframe <- cbind.data.frame(dframe,matrix(NA,ncol=ncol(fci),nrow=nrow(dframe)))
for (i in 1:ncol(fci)){
dframe[dframe[,1] %in% dates[[i]] , i+1] <- fci[[i]][dates[[i]] %in% dframe[,1]]
}
colnames(dframe) <- c("Dates",names)
## Next, we do "Countries included in fig_3.2"
name.range <- "A1:AJ1"
names <- read_excel("../../data/raw/nfci.xlsx",sheet = 2,range = name.range)
names.idx <- seq(1,ncol(names),7)
names <- gsub("^[0-9]. ",x=colnames(names)[names.idx],"")
data.range <- "A2:AN178"
data  <- read_excel("../../data/raw/nfci.xlsx",sheet = 2,range = data.range,)
idx <- names.idx + 4
dates <- data.frame( data[,names.idx] )
dates <-  lapply(1:length(dates),FUN=function(w) parse_date_time(dates[,w],orders="y!*q!*") )
fci   <-  data[, idx]
ncol.pre <- ncol(dframe)
dframe <- cbind.data.frame(dframe,matrix(NA,ncol=ncol(fci),nrow=nrow(dframe)))
ncol.pos <- ncol(dframe)
for (i in 1:ncol(fci)){
dframe[parse_date_time(dframe[,1],'yq') %in% dates[[i]] , ncol.pre + i] <-
fci[[i]][dates[[i]] %in% parse_date_time(dframe[,1],'yq')]
}
colnames(dframe) <- c(colnames(dframe)[1:ncol.pre] , names)
## Sorting by names in our sample.
cnames <- (read.csv('../../data/raw/keep_countries.csv',header = T))
cnames <- colnames(cnames)
# Translate IMF names and find intersection
imf.names <- countrycode(colnames(dframe[,-1]),origin = 'country.name',destination = 'iso3c')
nfci      <- data.frame(matrix(NA,nrow=nrow(dframe),ncol=length(cnames)))
colnames(nfci) <- cnames
for (j in 2:ncol(dframe)){
country       <- countrycode(colnames(dframe)[j],origin = 'country.name', destination = 'iso3c')
if (country %in% cnames) nfci[country] <- dframe[,j]
}
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
matplot(dframe[,-1])
sd(dframe[,-1])
sds(dframe[,-1])
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/NFCI.R', echo=TRUE)
rm(list=ls())
library(lubridate)
library(dplyr)
d <-  read.csv('../../data/raw/corporate_bond/BAA10Y.csv',stringsAsFactors = F)
rm(list=ls())
library(lubridate)
library(dplyr)
d <-  read.csv('../../data/raw/BAA10Y.csv',stringsAsFactors = F)
# Interpolate missing
baa      <- as.numeric(d$BAA10Y)
rm(list=ls())
library(lubridate)
library(dplyr)
d <-  read.csv('../../data/raw/BAA10Y.csv',stringsAsFactors = F)
# Interpolate missing
baa      <- as.numeric(d$BAA10Y)
i=0
while( sum(is.na(baa))>0 ){
i        = i+1
nas      <- which(is.na(baa))
baa[nas] <- (baa[nas-i]  + baa[nas+i])/2
}
dframe <- cbind.data.frame('date'= parse_date_time(as.character(d$DATE[1:length(d$DATE)]),orders = 'Y!-m!*-d!'),
'ret' = (baa) )
quarterly <- dframe %>% group_by(dates=ceiling_date(date, "quarter")) %>%
summarize(ret=tail(ret,1))
source('~/Desktop/projects/gar-replication/code/data-prep/construct_corporate_spread.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/construct_corporate_spread.R', echo=TRUE)
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
lt <- read.csv('../../data/raw/lt_interest.csv')
st <- read.csv('../../data/raw/st_interest.csv')
# Get country names
ctr_lt  <- unique(as.character(lt$LOCATION))
ctr_st  <- unique(as.character(st$LOCATION))
ctr_smp <- names(read.csv('../../data/names.csv'))
ctr_smp <- names(read.csv('../../data/raw/names.csv'))
ctr_smp <- names(read.csv('../../data/raw/keep_countries.csv'))
# Take US to set lengths
d_st      <- st[st$LOCATION=="USA",]
d_lt      <- lt[lt$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe_st <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_st) <- c('Dates',ctr_smp)
dframe_st[,'Dates'] <- as.character(d_st$TIME)
dframe_lt <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_lt) <- c('Dates',ctr_smp)
dframe_lt[,'Dates'] <- as.character(d_st$TIME)
for( i in 1:( length(ctr_smp) )){
d_st   <- st[ st$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_st$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_st[ dframe_st[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_st$Value[ date %in% common.dates ]
d_lt   <- lt[ lt$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_lt$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_lt[ dframe_lt[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_lt$Value[ date %in% common.dates ]
}
# Spread
d_spread <- dframe_lt[,-1] - dframe_st[,-1]
# Filling out NAs
d_spread <- scale(d_spread)
matplot(d_spread)
# Filling out NAs
d_spread <- scale(d_spread)
global   <- scale(rowMeans(d_spread,na.rm=TRUE))
for (i in 1:length(ctr_smp)){
d_spread[is.na(d_spread[,i]),i] <- scale(global[is.na(d_spread[,i])]) * sd(d_spread[,i],na.rm=T)
}
# Dates
dates_out <-  parse_date_time( dframe_st[,1] , order='y-q')
d_spread  <- cbind.data.frame('Dates'= dates_out,d_spread)
d_spread
matplot(d_spread)
View(d_spread)
# Dates
dates_out <-  parse_date_time( dframe_st[,1] , order='y-q')
d_spread  <- cbind.data.frame('Dates'= dates_out,d_spread)
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
lt <- read.csv('../../data/raw/lt_interest.csv')
st <- read.csv('../../data/raw/st_interest.csv')
# Get country names
ctr_lt  <- unique(as.character(lt$LOCATION))
ctr_st  <- unique(as.character(st$LOCATION))
ctr_smp <- names(read.csv('../../data/raw/keep_countries.csv'))
# Take US to set lengths
d_st      <- st[st$LOCATION=="USA",]
d_lt      <- lt[lt$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe_st <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_st) <- c('Dates',ctr_smp)
dframe_st[,'Dates'] <- as.character(d_st$TIME)
dframe_lt <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_lt) <- c('Dates',ctr_smp)
dframe_lt[,'Dates'] <- as.character(d_st$TIME)
for( i in 1:( length(ctr_smp) )){
d_st   <- st[ st$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_st$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_st[ dframe_st[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_st$Value[ date %in% common.dates ]
d_lt   <- lt[ lt$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_lt$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_lt[ dframe_lt[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_lt$Value[ date %in% common.dates ]
}
# Spread
d_spread <- dframe_lt[,-1] - dframe_st[,-1]
# Filling out NAs
d_spread <- scale(d_spread)
global   <- scale(rowMeans(d_spread,na.rm=TRUE))
for (i in 1:length(ctr_smp)){
d_spread[is.na(d_spread[,i]),i] <- scale(global[is.na(d_spread[,i])]) * sd(d_spread[,i],na.rm=T)
}
# Dates
dates_out <-  parse_date_time( dframe_st[,1] , order='y-q')
d_spread  <- cbind.data.frame('Dates'= dates_out,d_spread)
View(d_spread)
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
lt <- read.csv('../../data/raw/lt_interest.csv')
st <- read.csv('../../data/raw/st_interest.csv')
# Get country names
ctr_lt  <- unique(as.character(lt$LOCATION))
ctr_st  <- unique(as.character(st$LOCATION))
ctr_smp <- names(read.csv('../../data/raw/keep_countries.csv'))
# Take US to set lengths
d_st      <- st[st$LOCATION=="USA",]
d_lt      <- lt[lt$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe_st <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_st) <- c('Dates',ctr_smp)
dframe_st[,'Dates'] <- as.character(d_st$TIME)
dframe_lt <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_lt) <- c('Dates',ctr_smp)
dframe_lt[,'Dates'] <- as.character(d_st$TIME)
for( i in 1:( length(ctr_smp) )){
d_st   <- st[ st$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_st$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_st[ dframe_st[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_st$Value[ date %in% common.dates ]
d_lt   <- lt[ lt$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_lt$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_lt[ dframe_lt[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_lt$Value[ date %in% common.dates ]
}
# Spread
d_spread <- dframe_lt[,-1] - dframe_st[,-1]
# Filling out NAs
d_spread <- scale(d_spread)
global   <- scale(rowMeans(d_spread,na.rm=TRUE))
for (i in 1:length(ctr_smp)){
d_spread[is.na(d_spread[,i]),i] <- scale(global[is.na(d_spread[,i])]) * sd(d_spread[,i],na.rm=T)
}
# Dates
dates_out <-  parse_date_time( dframe_st[,1] , order='y-q')
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
lt <- read.csv('../../data/raw/lt_interest.csv')
st <- read.csv('../../data/raw/st_interest.csv')
lt <- lt$FREQUENCY=='Q'
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
lt <- read.csv('../../data/raw/lt_interest.csv')
st <- read.csv('../../data/raw/st_interest.csv')
lt <- lt[lt$FREQUENCY=='Q',]
st <- st[st$FREQUENCY=='Q',]
# Get country names
ctr_lt  <- unique(as.character(lt$LOCATION))
ctr_st  <- unique(as.character(st$LOCATION))
ctr_smp <- names(read.csv('../../data/raw/keep_countries.csv'))
# Take US to set lengths
d_st      <- st[st$LOCATION=="USA",]
d_lt      <- lt[lt$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe_st <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_st) <- c('Dates',ctr_smp)
dframe_st[,'Dates'] <- as.character(d_st$TIME)
dframe_lt <- data.frame( matrix(NA, nrow = nrow(d_st) ,ncol=length(ctr_smp)+1))
colnames(dframe_lt) <- c('Dates',ctr_smp)
dframe_lt[,'Dates'] <- as.character(d_st$TIME)
for( i in 1:( length(ctr_smp) )){
d_st   <- st[ st$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_st$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_st[ dframe_st[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_st$Value[ date %in% common.dates ]
d_lt   <- lt[ lt$LOCATION == ctr_smp[ i ] , ]
date   <- as.character(d_lt$TIME,orders="y-q")
common.dates <- intersect(date,dframe_st[,1])
dframe_lt[ dframe_lt[ , 'Dates'] %in% common.dates, ctr_smp[ i ] ] <- d_lt$Value[ date %in% common.dates ]
}
# Spread
d_spread <- dframe_lt[,-1] - dframe_st[,-1]
# Filling out NAs
d_spread <- scale(d_spread)
global   <- scale(rowMeans(d_spread,na.rm=TRUE))
for (i in 1:length(ctr_smp)){
d_spread[is.na(d_spread[,i]),i] <- scale(global[is.na(d_spread[,i])]) * sd(d_spread[,i],na.rm=T)
}
# Dates
dates_out <-  parse_date_time( dframe_st[,1] , order='y-q')
d_spread  <- cbind.data.frame('Dates'= dates_out,d_spread)
write.csv(d_spread,'../../data/clean/ts.csv')
rm(list=ls())
library(dplyr)
library(lubridate)
library(readxl)
d <- read_excel('../../data/final/gpr/gpr_web_latest.xlsx')
rm(list=ls())
library(dplyr)
library(lubridate)
library(readxl)
d <- read_excel('../../data/raw/gpr.xlsx')
dates <- as.Date(as.numeric( d$Date[1:(length(d$Date)-3)]) ,origin = "1899-12-30")
gpr   <- d$GPR
dframe <- cbind.data.frame('date' = dates,'gpr' = unname(d[1:(nrow(d)-3),2]) )
quarterly <- dframe %>% group_by(dates=floor_date(date, "quarter")) %>%
summarize(gpr=mean(gpr)/100)
quarterly <- quarterly[quarterly$dates > "1972-11-01" & quarterly$dates < "2017-01-01" ,]
quarterly <- quarterly[sapply(1:nrow(quarterly),function(w) !any(is.na(quarterly[w,]))),]
write.csv(quarterly,'../data/clean/gpr.csv')
source('~/Desktop/projects/gar-replication/code/data-prep/construct_gpr.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/construct_gpr.R', echo=TRUE)
rm(list=ls())
library(dplyr)
library(lubridate)
library(readxl)
rm(list=ls())
library(dplyr)
library(lubridate)
library(readxl)
files <- dir('../../data/raw/epu/')
files <- files[grepl('.xlsx',files)]
# Set dataframe
dfout <- data.frame(read_excel('../../data/final/epu/CAN.xlsx'))
# Set dataframe
dfout <- data.frame(read_excel('../../data/raw/epu/CAN.xlsx'))
dates <- paste0('01/',dfout[,2],'/',dfout[,1])
dates <- dates[1:(length(dates))]
dfout <- cbind.data.frame('date' = parse_date_time(as.character(dates),orders = 'd!/m!*/Y!')
,'epu' = unname(dfout[1:(nrow(dfout)),3]) )
dfout <- data.frame(dfout %>% group_by(dates=floor_date(date, "quarter")) %>%
summarize(epu=mean(epu)))
cnames <- names(read.csv('../../data/raw/keep_countries.csv'))
df <- data.frame(matrix(NA,nrow=nrow(dfout),ncol=length(cnames)+1))
colnames(df) <- c('Dates',cnames)
df[,1] <- dfout$dates
for(j in 1:length(files)){
file=files[j]
d <- data.frame( read_excel(sprintf('../../data/final/epu/%s',file)) )
name <- gsub('.xlsx','',file)
if (ncol(d) == 3){
dates <- paste0('01/',d[,2],'/',d[,1])
dates <- dates[1:(length(dates))]
dframe <- cbind.data.frame('date' = parse_date_time(as.character(dates),orders = 'd!/m!*/Y!')
,'epu' = unname(d[1:(nrow(d)),3]) )
quarterly <- data.frame(dframe %>% group_by(dates=floor_date(date, "quarter")) %>%
summarize(epu=mean(epu)))
} else if (ncol(d) == 2){
dates <- d[,1]
dframe <- cbind.data.frame('date' = parse_date_time(as.character(dates),orders = 'Y!/m!*/d!')
,'epu' = unname(d[1:(nrow(d)),2]) )
quarterly <- data.frame(dframe %>% group_by(dates=floor_date(date, "quarter")) %>%
summarize(epu=mean(epu)))
}
if (sum((df[,1] %in% quarterly$dates )) ==0 ) cat(sprintf('error at %s',name))
df[ which(df[,1] %in% quarterly$dates ), 1 +which(cnames==name)] <- quarterly$epu[which(quarterly$dates %in% df[,1] )]
}
for(j in 1:length(files)){
file=files[j]
d <- data.frame( read_excel(sprintf('../../data/raw/epu/%s',file)) )
name <- gsub('.xlsx','',file)
if (ncol(d) == 3){
dates <- paste0('01/',d[,2],'/',d[,1])
dates <- dates[1:(length(dates))]
dframe <- cbind.data.frame('date' = parse_date_time(as.character(dates),orders = 'd!/m!*/Y!')
,'epu' = unname(d[1:(nrow(d)),3]) )
quarterly <- data.frame(dframe %>% group_by(dates=floor_date(date, "quarter")) %>%
summarize(epu=mean(epu)))
} else if (ncol(d) == 2){
dates <- d[,1]
dframe <- cbind.data.frame('date' = parse_date_time(as.character(dates),orders = 'Y!/m!*/d!')
,'epu' = unname(d[1:(nrow(d)),2]) )
quarterly <- data.frame(dframe %>% group_by(dates=floor_date(date, "quarter")) %>%
summarize(epu=mean(epu)))
}
if (sum((df[,1] %in% quarterly$dates )) ==0 ) cat(sprintf('error at %s',name))
df[ which(df[,1] %in% quarterly$dates ), 1 +which(cnames==name)] <- quarterly$epu[which(quarterly$dates %in% df[,1] )]
}
source('~/Desktop/projects/gar-replication/code/data-prep/construct_epu.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/construct_epu.R', echo=TRUE)
source('~/Desktop/projects/gar-replication/code/data-prep/construct_epu.R', echo=TRUE)
d     <- read_excel('../../data/raw/wui.xlsx',sheet='T2')
rm(list=ls())
library(lubridate)
library(readxl)
d     <- read_excel('../../data/raw/wui.xlsx',sheet='T2')
dates <- parse_date_time(d$year,orders = 'y!*q!*')
cnames <- names(read.csv('../../data/keep_countries.csv'))
rm(list=ls())
library(lubridate)
library(readxl)
d     <- read_excel('../../data/raw/wui.xlsx',sheet='T2')
dates <- parse_date_time(d$year,orders = 'y!*q!*')
cnames <- names(read.csv('../../data/raw/keep_countries.csv'))
dmatched <- data.frame(d[,colnames(d) %in% cnames])
cnames[which(!(cnames %in% colnames(d) ))] # To see whos out
dout <- data.frame(matrix(NA,nrow=nrow(d),ncol=length(cnames)+1))
colnames(dout) <- c('Dates',cnames)
global <- rowMeans(dmatched,na.rm = T)
dout[ , colnames(dout) %in% colnames(dmatched) ] <- dmatched
dout[ , !(colnames(dout) %in% colnames(dmatched)) ] <- global
dout$Dates <- dates
write.csv(dout,'../../data/clean/wui.csv')
source('~/Desktop/projects/gar-replication/code/data-prep/construct_wui.R', echo=TRUE)
rm(list=ls())
library(lubridate)
rm(list=ls())
library(lubridate)
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
data <- read.csv('../../data/raw/gdp.csv')
data <- data[data$FREQUENCY=='Q',] # Keep quarterly Data
data <- data[data$MEASURE=='PC_CHGPP',]       # Keep quarter on quarter
# Get country names
countries <- unique(as.character(data$LOCATION))
# Create data frame with N+1 to keep dates.
dframe <- data.frame( matrix(NA, nrow = nrow(d) ,ncol=length(countries)+1))
colnames(dframe) <- c('Dates',countries)
dframe[,'Dates'] <- as.character(d$TIME)
for( i in 1:( length(countries) )){
d      <- data[ data$LOCATION == countries[ i ] , ]
date   <- as.character(d$TIME,orders="y-q")
common.dates <- intersect(date,dframe[,1])
dframe[ dframe[ , 'Dates'] %in% common.dates, i + 1 ] <- d$Value
}
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
data <- read.csv('../../data/raw/gdp.csv')
data <- data[data$FREQUENCY=='Q',] # Keep quarterly Data
data <- data[data$MEASURE=='PC_CHGPP',]       # Keep quarter on quarter
# Get country names
countries <- unique(as.character(data$LOCATION))
# Take US to set lengths
d      <- data[data$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe <- data.frame( matrix(NA, nrow = nrow(d) ,ncol=length(countries)+1))
colnames(dframe) <- c('Dates',countries)
dframe[,'Dates'] <- as.character(d$TIME)
for( i in 1:( length(countries) )){
d      <- data[ data$LOCATION == countries[ i ] , ]
date   <- as.character(d$TIME,orders="y-q")
common.dates <- intersect(date,dframe[,1])
dframe[ dframe[ , 'Dates'] %in% common.dates, i + 1 ] <- d$Value
}
if(!is.null(save.path)) write.csv(dframe, paste0(save.path , 'gdp.csv'))
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
data <- read.csv('../../data/raw/gdp.csv')
data <- data[data$FREQUENCY=='Q',] # Keep quarterly Data
data <- data[data$MEASURE=='PC_CHGPP',]       # Keep quarter on quarter
# Get country names
countries <- unique(as.character(data$LOCATION))
# Take US to set lengths
d      <- data[data$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe <- data.frame( matrix(NA, nrow = nrow(d) ,ncol=length(countries)+1))
colnames(dframe) <- c('Dates',countries)
dframe[,'Dates'] <- as.character(d$TIME)
for( i in 1:( length(countries) )){
d      <- data[ data$LOCATION == countries[ i ] , ]
date   <- as.character(d$TIME,orders="y-q")
common.dates <- intersect(date,dframe[,1])
dframe[ dframe[ , 'Dates'] %in% common.dates, i + 1 ] <- d$Value
}
if(!is.null(save.path)) write.csv(dframe, paste0(save.path , 'gdp.csv'))
rm(list=ls())
# Require lubridate to parse dates easier.
library(lubridate)
# Set path to save data, if no saving required, set to NULL
save.path = '../../data/clean/'
# Load data from OECD.
data <- read.csv('../../data/raw/gdp.csv')
data <- data[data$FREQUENCY=='Q',] # Keep quarterly Data
data <- data[data$MEASURE=='PC_CHGPP',]       # Keep quarter on quarter
# Get country names
countries <- unique(as.character(data$LOCATION))
# Take US to set lengths
d      <- data[data$LOCATION=="USA",]
# Create data frame with N+1 to keep dates.
dframe <- data.frame( matrix(NA, nrow = nrow(d) ,ncol=length(countries)+1))
colnames(dframe) <- c('Dates',countries)
dframe[,'Dates'] <- as.character(d$TIME)
for( i in 1:( length(countries) )){
d      <- data[ data$LOCATION == countries[ i ] , ]
date   <- as.character(d$TIME,orders="y-q")
common.dates <- intersect(date,dframe[,1])
dframe[ dframe[ , 'Dates'] %in% common.dates, i + 1 ] <- d$Value
}
cnames <- read.csv('../../data/raw/keep_countries.csv')
cnames <- colnames(cnames)
colnames(dframe)
colnames(dframe) %in% cnames
colnames(dframe) %in% c('Dates',cnames)
dframe <- dframe[,colnames(dframe) %in% c('Dates',cnames)]
source('~/Desktop/projects/gar-replication/code/data-prep/construct_gdp.R', echo=TRUE)
